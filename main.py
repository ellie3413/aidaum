import streamlit as st
# Streamlit ì„¤ì •ì„ ê°€ì¥ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•¨
st.set_page_config(page_title="AI ë„êµ¬ ì¶”ì²œ", page_icon="ğŸŒŸ", layout="wide")

import pandas as pd
import json
import matplotlib.pyplot as plt
import os
import re
import time
from datetime import datetime
from dotenv import load_dotenv
from survey import questions, reset_survey, run_survey
from langchain.chains import RetrievalQA
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains.question_answering import load_qa_chain
from user_type import determine_user_type, get_user_type_description
from langchain.text_splitter import RecursiveCharacterTextSplitter


#========== í™˜ê²½ ë³€ìˆ˜ ë¡œë”© ==========
load_dotenv()
api_key = st.secrets("OPENAI_API_KEY")

# API í‚¤ ê²€ì¦
if not api_key:
    st.error("âŒ OpenAI API í‚¤ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()
elif api_key.startswith('"') and api_key.endswith('"'):
    # ë”°ì˜´í‘œê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
    api_key = api_key.strip('"')
    st.warning("API í‚¤ì—ì„œ ë”°ì˜´í‘œë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.")

# íŠ¹ìˆ˜ ë¬¸ì ë° ë¹„ASCII ë¬¸ì ì²˜ë¦¬
api_key = re.sub(r'[^\x00-\x7F]+', '', api_key)  # ë¹„ASCII ë¬¸ì ì œê±°
api_key = re.sub(r'[^\w\-\.]', '', api_key)      # ì˜ë¬¸ì, ìˆ«ì, í•˜ì´í”ˆ, ì ë§Œ ìœ ì§€
    
# API í‚¤ ì •ë³´ í‘œì‹œ (ë””ë²„ê¹…ìš©)
#st.info(f"API í‚¤: {api_key[:5]}...{api_key[-5:]} (ê¸¸ì´: {len(api_key)})")

os.environ["OPENAI_API_KEY"] = api_key

# OpenAI API í‚¤ê°€ ìœ íš¨í•œì§€ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸
try:
    # OpenAI ê°ì²´ ìƒì„± í…ŒìŠ¤íŠ¸
    test_llm = OpenAI(temperature=0.1)
    #st.success("âœ… OpenAI API í‚¤ê°€ ìœ íš¨í•©ë‹ˆë‹¤.")
except Exception as e:
    st.error(f"âŒ OpenAI API í‚¤ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    st.stop()

#========== í•¨ìˆ˜ ì •ì˜ ==========
def load_json_data():
    """JSON íŒŒì¼ì—ì„œ AI ë„êµ¬ ë°ì´í„° ë¡œë“œ"""
    try:
        # UTF-8ë¡œ ì‹œë„(í•œêµ­ì–´ë¡œ ã„±ã„±ã„±)
        with open("tools.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except UnicodeDecodeError:
        # UTF-8ë¡œ ì‹¤íŒ¨í•œ ê²½ìš° errors='ignore' ì˜µì…˜ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„
        try:
            with open("tools.json", "r", encoding="utf-8", errors="ignore") as f:
                return json.load(f)
        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ ì‹œ latin-1 ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„
            try:
                with open("tools.json", "r", encoding="latin-1") as f:
                    return json.load(f)
            except Exception as e:
                st.error(f"âŒ JSON íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
                return []
    except Exception as e:
        st.error(f"âŒ JSON íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

def filter_tools_by_difficulty(tools, difficulty_level):
    """ë‚œì´ë„ ê¸°ì¤€ìœ¼ë¡œ AI ë„êµ¬ í•„í„°ë§"""
    if difficulty_level == "ëª¨ë“  ë‚œì´ë„":
        return tools
    
    difficulty_map = {
        "ì‰¬ì›€": "low", 
        "ì¤‘ê°„": "medium", 
        "ì–´ë ¤ì›€": "hard"
    }
    
    target_difficulty = difficulty_map.get(difficulty_level)
    
    filtered = []
    for tool in tools:
        # None ë‚œì´ë„ëŠ” ì¤‘ê°„ ë‚œì´ë„ë¡œ ê°„ì£¼
        if tool.get("difficulty") is None and target_difficulty == "medium":
            filtered.append(tool)
        elif tool.get("difficulty") == target_difficulty:
            filtered.append(tool)
    return filtered

def filter_tools_by_category(tools, category):
    """ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ìœ¼ë¡œ AI ë„êµ¬ í•„í„°ë§"""
    if category == "ëª¨ë“  ì¹´í…Œê³ ë¦¬":
        return tools
    
    return [tool for tool in tools if tool.get("category") == category]

def filter_tools_by_search(tools, search_term):
    """ê²€ìƒ‰ì–´ ê¸°ì¤€ìœ¼ë¡œ AI ë„êµ¬ í•„í„°ë§"""
    if not search_term:
        return tools
    
    return [tool for tool in tools if search_term.lower() in tool.get("name", "").lower() or 
            (tool.get("description") and search_term.lower() in tool.get("description", "").lower())]

def get_tool_details(tool_name, tools_data):
    """ë„êµ¬ ì´ë¦„ìœ¼ë¡œ ì„¸ë¶€ ì •ë³´ ê²€ìƒ‰"""
    for tool in tools_data:
        if tool["name"].lower() == tool_name.lower():
            return tool
    return None

def find_best_matching_tool(tool_name, tools_data):
    """ê°€ì¥ ìœ ì‚¬í•œ ë„êµ¬ ì´ë¦„ ì°¾ê¸°"""
    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë„êµ¬ ë¨¼ì € í™•ì¸
    for tool in tools_data:
        if tool["name"].lower() == tool_name.lower():
            return tool
    
    # ë¶€ë¶„ ì¼ì¹˜í•˜ëŠ” ë„êµ¬ í™•ì¸
    for tool in tools_data:
        if tool_name.lower() in tool["name"].lower() or tool["name"].lower() in tool_name.lower():
            return tool
    
    return None

def save_user_feedback(tool_name, rating, feedback_text):
    """ì‚¬ìš©ì í”¼ë“œë°± ì €ì¥"""
    feedback_data = {
        "tool": tool_name,
        "rating": rating,
        "feedback": feedback_text,
        "responses": st.session_state.responses
    }
    
    # í”¼ë“œë°± íŒŒì¼ ì €ì¥
    try:
        if os.path.exists("user_feedback.json"):
            try:
                with open("user_feedback.json", "r", encoding="utf-8") as f:
                    existing_data = json.load(f)
            except UnicodeDecodeError:
                with open("user_feedback.json", "r", encoding="latin-1") as f:
                    existing_data = json.load(f)
            
            existing_data.append(feedback_data)
            with open("user_feedback.json", "w", encoding="utf-8", errors="ignore") as f:
                json.dump(existing_data, f, ensure_ascii=True, indent=2)
        else:
            with open("user_feedback.json", "w", encoding="utf-8", errors="ignore") as f:
                json.dump([feedback_data], f, ensure_ascii=True, indent=2)
        return True
    except Exception as e:
        st.error(f"í”¼ë“œë°± ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def visualize_category_distribution(tools_data):
    """ì¹´í…Œê³ ë¦¬ë³„ AI ë„êµ¬ ë¶„í¬ ì‹œê°í™”"""
    categories = {}
    for tool in tools_data:
        category = tool.get("category", "ê¸°íƒ€")
        if category in categories:
            categories[category] += 1
        else:
            categories[category] = 1
    
    # ìƒìœ„ 10ê°œ ì¹´í…Œê³ ë¦¬ë§Œ í‘œì‹œ
    sorted_categories = dict(sorted(categories.items(), key=lambda x: x[1], reverse=True)[:10])
    
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(sorted_categories.keys(), sorted_categories.values(), color='skyblue')
    
    # ê°’ ë ˆì´ë¸” í‘œì‹œ
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{height}',
                ha='center', va='bottom', fontsize=9)
    
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    return fig

def recommend_tools_by_criteria(tools_data, user_responses, max_recommendations=3):
    """ì‚¬ìš©ì ì‘ë‹µ ê¸°ë°˜ìœ¼ë¡œ AI ë„êµ¬ ì•Œê³ ë¦¬ì¦˜ì  ì¶”ì²œ"""
    if not tools_data:
        return []
    
    # ì ìˆ˜ ì´ˆê¸°í™”
    for tool in tools_data:
        tool['score'] = 0
        
    # 1. ë‚œì´ë„ ê¸°ì¤€ ì ìˆ˜í™” (ì‚¬ìš©ì ì„ í˜¸ ë‚œì´ë„ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
    difficulty_map = {
        "ì‰¬ì›€ (ì´ˆë³´ìë„ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬)": "low",
        "ì¤‘ê°„ (ê¸°ë³¸ì ì¸ ì§€ì‹ì´ í•„ìš”í•œ ë„êµ¬)": "medium", 
        "ì–´ë ¤ì›€ (ì „ë¬¸ì ì¸ ì§€ì‹ì´ í•„ìš”í•œ ê³ ê¸‰ ë„êµ¬)": "hard",
        "ë‚œì´ë„ë³´ë‹¤ëŠ” ê¸°ëŠ¥ ì¤‘ì‹¬ìœ¼ë¡œ ì„ íƒí•˜ê³  ì‹¶ìŒ": None
    }
    
    preferred_difficulty = difficulty_map.get(
        user_responses.get('preferred_difficulty', "ë‚œì´ë„ë³´ë‹¤ëŠ” ê¸°ëŠ¥ ì¤‘ì‹¬ìœ¼ë¡œ ì„ íƒí•˜ê³  ì‹¶ìŒ")
    )
    
    # ë‚œì´ë„ ì ìˆ˜ ê³„ì‚°
    if preferred_difficulty:
        for tool in tools_data:
            if tool.get('difficulty') == preferred_difficulty:
                tool['score'] += 5
            # None ë‚œì´ë„ëŠ” mediumìœ¼ë¡œ ê°„ì£¼
            elif tool.get('difficulty') is None and preferred_difficulty == "medium":
                tool['score'] += 4
    
    # 2. AI
    # 2. AI ì§€ì‹ ìˆ˜ì¤€ì— ë”°ë¥¸ ë‚œì´ë„ ì¡°ì •
    knowledge_level = user_responses.get('ai_knowledge', '')
    if knowledge_level in ['ì „í˜€ ëª¨ë¥¸ë‹¤', 'ì´ë¦„ë§Œ ë“¤ì–´ë´¤ë‹¤']:
        # ì´ˆë³´ìëŠ” ì‰¬ìš´ ë„êµ¬ ì„ í˜¸
        for tool in tools_data:
            if tool.get('difficulty') == 'low':
                tool['score'] += 3
    elif knowledge_level in ['AI ëª¨ë¸ì´ë‚˜ ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ ë‹¤ë¤„ë³¸ ì  ìˆë‹¤']:
        # ì „ë¬¸ê°€ëŠ” ì–´ë ¤ìš´ ë„êµ¬ ì„ í˜¸
        for tool in tools_data:
            if tool.get('difficulty') == 'hard':
                tool['score'] += 3
    
    # 3. ê´€ì‹¬ ë¶„ì•¼ ê¸°ë°˜ ì ìˆ˜í™”
    interests = user_responses.get('tool_interest', [])
    interest_category_map = {
        "í…ìŠ¤íŠ¸ ìƒì„±": ["AI Assistants (Chatbots)", "Writing", "Grammar and Writing Improvement"],
        "ì´ë¯¸ì§€ ìƒì„±": ["Image Generation", "Graphic Design"],
        "ì˜ìƒ/ìŒì„± í•©ì„±": ["Video Generation and Editing", "Voice Generation", "Music Generation"],
        "ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”": ["Research"],
        "ì—…ë¬´ ìë™í™”": ["Project Management", "Scheduling", "Email"],
        "ê²€ìƒ‰ ë° ì§€ì‹ ê´€ë¦¬": ["Search Engines", "Knowledge Management"],
        "ì½”ë“œ ìƒì„± ë° ê°œë°œ ì§€ì›": ["App Builders & Coding"],
        "ë²ˆì—­ ë° ì–¸ì–´ í•™ìŠµ": ["Grammar and Writing Improvement","AI Assistants (Chatbots)"],
        "ê¸°íƒ€": []
    }
    
    for interest in interests:
        matching_categories = interest_category_map.get(interest, [])
        for tool in tools_data:
            if tool.get('category') in matching_categories:
                tool['score'] += 4
    
    # 4. íŠ¹ì • ëª©ì  ê¸°ë°˜ ì ìˆ˜í™”
    purposes = user_responses.get('specific_purpose', [])
    purpose_category_map = {
        "ë¬¸ì„œ ì‘ì„± ë° í¸ì§‘": ["Writing", "Grammar and Writing Improvement","AI Assistants (Chatbots)"],
        "ì´ë¯¸ì§€/ì˜ìƒ ì œì‘": ["Image Generation", "Video Generation and Editing","Graphic Design"],
        "ë°ì´í„° ë¶„ì„": ["Research"],
        "í”„ë¡œê·¸ë˜ë° ë° ê°œë°œ": ["App Builders & Coding"],
        "ì˜ë¬¸ì´ë ¥ì„œ ì‘ì„±": ["Resume Builders", "Writing", "AI Assistants (Chatbots)"],
        "ë§ˆì¼€íŒ… ë° í™ë³´": ["Marketing", "Social Media Management"],
        "êµìœ¡ ë° í•™ìŠµ": ["Knowledge Management","Search Engines"],
        "ì—…ë¬´ ìë™í™”": ["Project Management", "Scheduling", "Email"],
        "ê³ ê° ì„œë¹„ìŠ¤": ["Customer Service"],
        "ì—°êµ¬ ë° ë…¼ë¬¸ ì‘ì„±": ["Research", "Writing","Search Engines","AI Assistants (Chatbots)"],
        "ê¸°íƒ€": []
    }
    
    for purpose in purposes:
        matching_categories = purpose_category_map.get(purpose, [])
        for tool in tools_data:
            if tool.get('category') in matching_categories:
                tool['score'] += 4
    
    # 5. ì§ì—… ê¸°ë°˜ ì ìˆ˜í™” (ì¶”ê°€ë¨)
    job = user_responses.get('job', '')
    job_category_map = {
        "í•™ìƒ": ["Writing", "Research", "Grammar and Writing Improvement", "Knowledge Management"],
        "ê°œë°œì/IT ì¢…ì‚¬ì": ["App Builders & Coding", "AI Assistants (Chatbots)"],
        "êµìœ¡ì/ì—°êµ¬ì›": ["Research", "Knowledge Management", "Presentations", "Writing"],
        "ë””ìì´ë„ˆ/ì°½ì‘ì": ["Image Generation", "Video Generation and Editing", "Graphic Design", "Music Generation"],
        "ë§ˆì¼€í„°/í™ë³´": ["Social Media Management", "Marketing", "Writing", "Image Generation"],
        "ì‚¬ë¬´ì§": ["Email", "Project Management", "Scheduling", "Writing"],
        "ê²½ì˜/ê´€ë¦¬ì": ["Project Management", "Knowledge Management", "Presentations"],
        "ì°½ì—…ê°€/í”„ë¦¬ëœì„œ": ["Marketing", "Social Media Management", "Email", "Customer Service"],
        "ì˜ë£Œ/ê±´ê°• ì¢…ì‚¬ì": ["Research", "Knowledge Management"],
        "ë²•ë¥ /ê¸ˆìœµ ì „ë¬¸ê°€": ["Research", "Grammar and Writing Improvement", "Writing"],
        "ê¸°íƒ€": []
    }
    
    if job:
        matching_categories = job_category_map.get(job, [])
        for tool in tools_data:
            if tool.get('category') in matching_categories:
                tool['score'] += 5  # ì§ì—… ê´€ë ¨ì„±ì´ ë†’ì€ ë„êµ¬ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    
    # ë„êµ¬ ì„¤ëª…ì´ ìˆëŠ” ë„êµ¬ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
    for tool in tools_data:
        if tool.get('description') and len(str(tool.get('description'))) > 10:
            tool['score'] += 1
    
    # ìµœì¢… ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ìƒìœ„ ì¶”ì²œ
    sorted_tools = sorted(tools_data, key=lambda x: x.get('score', 0), reverse=True)
    
    # ìµœì†Œí•œ í•˜ë‚˜ì˜ ì‰¬ìš´ ë„êµ¬ê°€ í¬í•¨ë˜ë„ë¡ ë³´ì¥ (ì´ˆë³´ìë¥¼ ìœ„í•œ ë°°ë ¤)
    recommended = []
    has_easy_tool = False
    
    # ìƒìœ„ ë„êµ¬ë“¤ ì¤‘ì—ì„œ ì„ íƒ
    for tool in sorted_tools:
        if len(recommended) < max_recommendations:
            recommended.append(tool)
            if tool.get('difficulty') == 'low':
                has_easy_tool = True
        elif not has_easy_tool and tool.get('difficulty') == 'low':
            # ì‰¬ìš´ ë„êµ¬ê°€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ë„êµ¬ë¥¼ ì‰¬ìš´ ë„êµ¬ë¡œ êµì²´
            recommended[-1] = tool
            has_easy_tool = True
            break
    
    return recommended

def translate_difficulty(difficulty):
    """ë‚œì´ë„ ì˜ì–´ í‘œí˜„ì„ í•œêµ­ì–´ë¡œ ë³€í™˜"""
    if difficulty == "low":
        return "ì‰¬ì›€"
    elif difficulty == "medium":
        return "ì¤‘ê°„"
    elif difficulty == "hard":
        return "ì–´ë ¤ì›€"
    return "ì¤‘ê°„"  # ê¸°ë³¸ê°’

def add_korean_description(tools):
    """ì˜ì–´ ì„¤ëª…ì´ ìˆëŠ” ë„êµ¬ì— í•œêµ­ì–´ ì„¤ëª… ì¶”ê°€"""
    korean_descriptions = {
        "ChatGPT": "ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìƒì„±ê³¼ ëŒ€í™”ê°€ ê°€ëŠ¥í•œ OpenAIì˜ ëŒ€í‘œì ì¸ AI ì±—ë´‡ìœ¼ë¡œ, ì½”ë”©, ê¸€ì“°ê¸°, ì§ˆë¬¸ ì‘ë‹µ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "Claude": "Anthropicì—ì„œ ê°œë°œí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œ, ì¹œì ˆí•˜ê³  ì •í™•í•œ ì‘ë‹µê³¼ íŠ¹íˆ ì½”ë”©ì— ê°•ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
        "Gemini": "Googleì—ì„œ ê°œë°œí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œ êµ¬ê¸€ ìƒíƒœê³„ì™€ ë†’ì€ í†µí•©ì„±ì„ ê°€ì§€ê³  ìˆìœ¼ë©° ê²€ìƒ‰ê³¼ ì •ë³´ ìš”ì•½ì— ê°•ì ì´ ìˆìŠµë‹ˆë‹¤.",
        "Midjourney": "í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” AI ë„êµ¬ë¡œ, ì˜ˆìˆ ì  í‘œí˜„ê³¼ ì°½ì˜ì ì¸ ì‹œê°í™”ì— íƒì›”í•©ë‹ˆë‹¤.",
        "Perplexity": "ë‹¤ì–‘í•œ ì •ë³´ ì†ŒìŠ¤ë¥¼ í™œìš©í•´ ê¹Šì´ ìˆëŠ” ê²€ìƒ‰ê³¼ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.",
        "Grammarly": "í…ìŠ¤íŠ¸ ì‘ì„± ì‹œ ë¬¸ë²•, ë§ì¶¤ë²•, ë¬¸ì²´ë¥¼ ìë™ìœ¼ë¡œ êµì •í•´ì£¼ëŠ” AI ê¸€ì“°ê¸° ë„ìš°ë¯¸ì…ë‹ˆë‹¤.",
        "Canva Magic Studio": "ì†ì‰¬ìš´ ë””ìì¸ ì œì‘ì„ ìœ„í•œ AI ê¸°ëŠ¥ì´ ê°•í™”ëœ ê·¸ë˜í”½ ë””ìì¸ í”Œë«í¼ì…ë‹ˆë‹¤.",
    }
    
    for tool in tools:
        if tool.get("name") in korean_descriptions and (tool.get("description") is None or "Korean" not in tool.get("lang", [])):
            tool["korean_description"] = korean_descriptions[tool.get("name")]
    
    return tools

def generate_expert_explanation_by_sections(tool_name, qa_system, st):
    """
    AI ë„êµ¬ ì „ë¬¸ê°€ì˜ ë„êµ¬ ì„¤ëª…ì„ ì„¹ì…˜ë³„ë¡œ ë‚˜ëˆ„ì–´ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    ê° ì„¹ì…˜ì„ ê°œë³„ì ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ì‘ë‹µì´ ì¤‘ê°„ì— ëŠê¸°ëŠ” ë¬¸ì œë¥¼ ë°©ì§€
    """
    # ì‚¬ìš©ì íƒ€ì…ì— ë§ì¶˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
    user_type = ""
    if 'responses' in st.session_state:
        job = st.session_state.responses.get('job', '')
        ai_knowledge = st.session_state.responses.get('ai_knowledge', '')
        
        if job:
            user_type += f" {job}ë¡œì„œ"
        
        if ai_knowledge in ['ì „í˜€ ëª¨ë¥¸ë‹¤', 'ì´ë¦„ë§Œ ë“¤ì–´ë´¤ë‹¤']:
            user_type += " AI ì´ˆë³´ìë¥¼ ìœ„í•œ"
        elif ai_knowledge in ['AI ëª¨ë¸ì´ë‚˜ ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ ë‹¤ë¤„ë³¸ ì  ìˆë‹¤']:
            user_type += " AI ì „ë¬¸ê°€ë¥¼ ìœ„í•œ"
    
    # ì„¹ì…˜ ì •ì˜
    sections = [
        {
            "emoji": "âœ¨",
            "title": f"{tool_name}ì´ë€?",
            "prompt": f"{tool_name}ì˜ í•µì‹¬ ê¸°ëŠ¥ê³¼ ëª©ì ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."
        },
        {
            "emoji": "ğŸš€",
            "title": "ì£¼ìš” ê¸°ëŠ¥",
            "prompt": f"{tool_name}ì˜ ê°€ì¥ ì¸ê¸° ìˆëŠ” 3-5ê°€ì§€ í•µì‹¬ ê¸°ëŠ¥ì„ ê°„ëµíˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        },
        {
            "emoji": "ğŸ”„",
            "title": "ìœ ì‚¬í•œ ëŒ€ì²´ ë„êµ¬",
            "prompt": f"{tool_name}ê³¼ ë¹„ìŠ·í•œ ê¸°ëŠ¥ì„ ê°€ì§„ ë‹¤ë¥¸ ë„êµ¬ 1-2ê°œì™€ ê°„ëµí•œ ë¹„êµë¥¼ í•´ì£¼ì„¸ìš”."
        }
    ]
    
    # ê° ì„¹ì…˜ë³„ë¡œ ì‘ë‹µ ìƒì„±
    for section in sections:
        st.markdown(f"### {section['emoji']} {section['title']}")
        with st.spinner(f"{section['title']} ì •ë³´ë¥¼ ìƒì„± ì¤‘..."):
            try:
                # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
                section_prompt = f"""
                ë‹¹ì‹ ì€ AI ë„êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.{user_type} ë‹¤ìŒ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
                
                {section["prompt"]}
                
                ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ, ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
                """
                
                # ì‘ë‹µ ìƒì„± (LangChain ë²„ì „ì— ë”°ë¼ run ë˜ëŠ” invoke ì‚¬ìš©)
                try:
                    # ìµœì‹  LangChain ë²„ì „ìš©
                    section_response = qa_system.invoke(section_prompt)
                    if isinstance(section_response, dict) and "result" in section_response:
                        section_result = section_response["result"]
                    else:
                        section_result = str(section_response)
                except:
                    # ì´ì „ LangChain ë²„ì „ìš©
                    section_result = qa_system.run(section_prompt)
                
                # ì‘ë‹µì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° ëŒ€ì²´ í…ìŠ¤íŠ¸ ì œê³µ
                if len(section_result.strip()) < 20:
                    section_result = f"{tool_name}ì— ëŒ€í•œ ì´ ì •ë³´ëŠ” í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¶©ë¶„íˆ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                # ì„¹ì…˜ ë‚´ìš© í‘œì‹œ
                st.markdown(section_result)
                
            except Exception as e:
                st.warning(f"{section['title']} ì •ë³´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                st.markdown(f"{tool_name}ì— ëŒ€í•œ ì´ ì •ë³´ëŠ” í˜„ì¬ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return "ì„¤ëª… ìƒì„± ì™„ë£Œ"


#========== Streamlit UI ==========
st.title("ğŸ›¸ ì—ì´ì•„ì´ë‹¤ì›€")
st.write("ì„¤ë¬¸ì¡°ì‚¬ë¥¼ ì™„ë£Œí•˜ì‹œë©´, ë‹¹ì‹ ì˜ AI ìœ í˜•ê³¼ í•„ìš”í•œ AI ë„êµ¬ë¥¼ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.")

#========== ì„¤ë¬¸ í™”ë©´ ==========
run_survey()

#========== ì„¤ë¬¸ ì™„ë£Œ ì—¬ë¶€ í™•ì¸ ==========
if not st.session_state.get("survey_complete", False):
    st.stop()

responses = st.session_state.responses

#========== tools.txt ë° JSON ë°ì´í„° ë¡œë“œ ==========

# PDF íŒŒì¼ ë¡œë“œ
with st.spinner("PDF íŒŒì¼ ë¡œë”© ì¤‘..."):
    try:
        # PDF ë¡œë” ìƒì„±
        pdf_loader = PyPDFLoader("tools.pdf")
        # PDF íŒŒì¼ì—ì„œ ë¬¸ì„œ ì¶”ì¶œ
        pages = pdf_loader.load()
        #st.success(f"âœ… PDF íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(pages)}í˜ì´ì§€")
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        # ë¬¸ì„œ ë¶„í• 
        split_docs = text_splitter.split_documents(pages)
        #st.success(f"âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(split_docs)}ê°œ ì²­í¬")
    except Exception as e:
        st.error(f"âŒ PDF íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()

# JSON ë°ì´í„° ë¡œë“œ
tools_data = load_json_data()

#========== ì‚¬ìš©ì ì„ í˜¸ë„ì— ë§ëŠ” ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜ ê²°ì • ==========
search_kwargs = {"k": 5}  # ê¸°ë³¸ê°’

# AI ì§€ì‹ ìˆ˜ì¤€ì— ë”°ë¼ ê²€ìƒ‰ ê¹Šì´ ì¡°ì •
if responses.get('ai_knowledge') in ['ì „í˜€ ëª¨ë¥¸ë‹¤', 'ì´ë¦„ë§Œ ë“¤ì–´ë´¤ë‹¤']:
    search_kwargs["k"] = 3  # ì´ˆë³´ìëŠ” ë” ê¸°ë³¸ì ì¸ ë‚´ìš©ë§Œ ê²€ìƒ‰
elif responses.get('ai_knowledge') in ['AI ëª¨ë¸ì´ë‚˜ ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ ë‹¤ë¤„ë³¸ ì  ìˆë‹¤']:
    search_kwargs["k"] = 7  # ì „ë¬¸ê°€ëŠ” ë” ê¹Šì€ ê²€ìƒ‰

#========== RAG ê¸°ë°˜ ë„êµ¬ ì¶”ì²œ ==========
with st.spinner("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘..."):
    try:
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings = OpenAIEmbeddings()
        
        # ìœ ë‹ˆì½”ë“œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¬¸ì„œ ì •ì œ
        for doc in split_docs:
            # ë¹„ASCII ë¬¸ì ì²˜ë¦¬
            doc.page_content = re.sub(r'[\u2014\u2013\u2015\u2017\u2018\u2019\u201a\u201b\u201c\u201d\u201e\u201f\u2020\u2021\u2026\u2032\u2033]+', '-', doc.page_content)
            # ë‚˜ë¨¸ì§€ íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ë¬¸ì ì²˜ë¦¬
            doc.page_content = doc.page_content.encode('ascii', errors='ignore').decode('ascii')
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = FAISS.from_documents(split_docs, embeddings)
        #st.success("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
        
        # RAG ì‹œìŠ¤í…œ ì„¤ì •
        qa = RetrievalQA.from_chain_type(
            llm=OpenAI(temperature=0.3),
            retriever=vectorstore.as_retriever(search_kwargs=search_kwargs)
        )
    except Exception as e:
        st.error(f"âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()

#========== AI ìœ í˜• ì¶”ì²œ ==========

st.markdown("### ğŸ§© ë‹¹ì‹ ì˜ AI ìœ í˜•ì€?")

# ì‚¬ìš©ì ìœ í˜• ê²°ì •
user_type = determine_user_type(responses)
user_type_info = get_user_type_description(user_type)

# ìœ í˜• ì •ë³´ í‘œì‹œ
st.markdown(f"## {user_type_info['title']}")
st.markdown(user_type_info['description'])

# ìœ í˜• ì„¸ë¶€ ì •ë³´
col1, col2 = st.columns(2)
with col1:
    st.markdown("#### ğŸ’ª ê°•ì ")
    st.markdown(user_type_info['strengths'])
    
with col2:
    st.markdown("#### ğŸš€ ì¶”ì²œ ì ‘ê·¼ë²•")
    st.markdown(user_type_info['recommended_approach'])

st.markdown("---")


#========== ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ë„êµ¬ ì¶”ì²œ ==========
st.markdown("### ğŸ” ë‹¹ì‹ ì„ ìœ„í•œ AI ë„êµ¬ ì¶”ì²œ")

# í•œêµ­ì–´ ì„¤ëª… ì¶”ê°€
tools_data = add_korean_description(tools_data)

# ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ì¶”ì²œ
with st.spinner("ì¶”ì²œ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
    recommended_tools = recommend_tools_by_criteria(tools_data, responses, max_recommendations=3)

# ì¶”ì²œ ê²°ê³¼ í‘œì‹œ
if recommended_tools:
    st.success(f"âœ… ë§ì¶¤í˜• AI ë„êµ¬ ì¶”ì²œ ì™„ë£Œ!")
    st.markdown(f"**{user_type}** ìœ í˜•ì¸ ë‹¹ì‹ ì„ ìœ„í•œ ë§ì¶¤í˜• AI ë„êµ¬ì…ë‹ˆë‹¤. ì´ ë„êµ¬ë“¤ì€ ì„¤ë¬¸ ì‘ë‹µì— ê¸°ë°˜í•˜ì—¬ ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬, ëª©ì , ì§ì—…ì„ ê³ ë ¤í•˜ì—¬ íŠ¹ë³„íˆ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")


    
    # 3ê°œì˜ ì—´ë¡œ ì¶”ì²œ ë„êµ¬ í‘œì‹œ
    cols = st.columns(len(recommended_tools))
    
    for i, tool in enumerate(recommended_tools):
        with cols[i]:
            st.markdown(f"### {i+1}. {tool.get('name')}")
            st.markdown(f"**ì¹´í…Œê³ ë¦¬**: {tool.get('category', 'ì •ë³´ ì—†ìŒ')}")
            st.markdown(f"**ë‚œì´ë„**: {translate_difficulty(tool.get('difficulty', 'medium'))}")
            
            # í•œêµ­ì–´ ì„¤ëª… ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ì„¤ëª… ì‚¬ìš©
            if tool.get("korean_description"):
                st.markdown(f"**ì„¤ëª…**: {tool.get('korean_description')}")
            elif tool.get("description"):
                st.markdown(f"**ì„¤ëª…**: {tool.get('description')}")
            else:
                st.markdown("**ì„¤ëª…**: ìƒì„¸ ì„¤ëª… ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë„êµ¬ë³„ ìƒì„¸ ì„¤ëª… ë²„íŠ¼
            if st.button(f"{tool.get('name')} ìì„¸íˆ ë³´ê¸°", key=f"detail_{i}"):
                # ì„¸ì…˜ ìƒíƒœì— ì„ íƒëœ ë„êµ¬ ì €ì¥
                st.session_state.selected_tool = tool.get('name')
                st.rerun()
else:
    st.warning("âš ï¸ ì¶”ì²œ ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ë¬¸ ì‘ë‹µì„ ì‹œë„í•´ ë³´ì„¸ìš”.")

# ì„ íƒëœ ë„êµ¬ ìƒì„¸ ì •ë³´ í‘œì‹œ
if hasattr(st.session_state, 'selected_tool') and st.session_state.selected_tool:
    tool_name = st.session_state.selected_tool
    st.markdown(f"## {tool_name} ìƒì„¸ ì •ë³´")
    
    # íˆ´ ì •ë³´ ì°¾ê¸°
    tool_info = get_tool_details(tool_name, tools_data)
    
    if tool_info:
        st.markdown(f"**ì¹´í…Œê³ ë¦¬**: {tool_info.get('category', 'ì •ë³´ ì—†ìŒ')}")
        st.markdown(f"**ë‚œì´ë„**: {translate_difficulty(tool_info.get('difficulty', 'medium'))}")
        
        # í•œêµ­ì–´ ì„¤ëª… ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ì„¤ëª… ì‚¬ìš©
        if tool_info.get("korean_description"):
            st.markdown(f"**ì„¤ëª…**: {tool_info.get('korean_description')}")
        elif tool_info.get("description"):
            st.markdown(f"**ì„¤ëª…**: {tool_info.get('description')}")
        else:
            st.markdown("**ì„¤ëª…**: ìƒì„¸ ì„¤ëª… ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
         # AI ë„êµ¬ ì „ë¬¸ê°€ì˜ ì„¤ëª… ìƒì„± (ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬)
        try:
            st.markdown("### ğŸ¤– AI ë„êµ¬ ì „ë¬¸ê°€ì˜ ìƒì„¸ ì„¤ëª…")
            with st.spinner(f"{tool_name}ì— ê´€í•œ ìƒì„¸ ì •ë³´ ë¶„ì„ ì¤‘..."):
                # ìƒˆë¡œìš´ í•¨ìˆ˜ í˜¸ì¶œ (ì„¹ì…˜ë³„ ìƒì„±)
                generate_expert_explanation_by_sections(tool_name, qa, st)
        
        except Exception as e:
            st.error(f"ì „ë¬¸ê°€ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.error(f"{tool_name}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìƒì„¸ë³´ê¸° ë‹«ê¸°
    if st.button("ìƒì„¸ ì •ë³´ ë‹«ê¸°"):
        del st.session_state.selected_tool
        st.rerun()

#========== ì ìˆ˜í‘œ ==========
if recommended_tools:
    with st.expander("ğŸ¤– ë§ì¶¤í˜• ë„êµ¬ ì„ ì • ê·¼ê±°(ì ìˆ˜í‘œ)", expanded=False):
        st.markdown("#### ì„¤ë¬¸ ë°”íƒ• ì ìˆ˜ ë¶„í¬")
        score_fig, ax = plt.subplots(figsize=(8, 4))
        tool_names = [tool.get('name') for tool in recommended_tools]
        scores = [tool.get('score', 0) for tool in recommended_tools]
        
        bars = ax.barh(tool_names, scores, color=['#2E86C1', '#3498DB', '#85C1E9'])

        # ê°’ í‘œì‹œ
        for i, (score, bar) in enumerate(zip(scores, bars)):
            ax.text(score + 0.5, i, f"{score}", ha='left', va='center')
        
        plt.xlabel('your score')
        plt.tight_layout()
        st.pyplot(score_fig)


#========== ë‚œì´ë„ í•„í„° ë° ì„¸ë¶€ ì •ë³´ ==========
st.markdown("---")
st.markdown("### ğŸ” AI ë„êµ¬ ë°ì´í„°ë² ì´ìŠ¤ íƒìƒ‰")

if tools_data:
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # ë‚œì´ë„ë³„ í•„í„°ë§
        difficulty_options = ["ëª¨ë“  ë‚œì´ë„", "ì‰¬ì›€", "ì¤‘ê°„", "ì–´ë ¤ì›€"]
        selected_difficulty = st.selectbox("ë‚œì´ë„ë³„ í•„í„°ë§", difficulty_options)
    
    with col2:
        # ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§
        categories = ["ëª¨ë“  ì¹´í…Œê³ ë¦¬"] + sorted(list(set([tool.get("category", "ê¸°íƒ€") for tool in tools_data if tool.get("category")])))
        selected_category = st.selectbox("ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§", categories)
    
    with col3:
        # ê²€ìƒ‰ì–´ í•„í„°ë§
        search_term = st.text_input("ğŸ” ë„êµ¬ ì´ë¦„ ë˜ëŠ” ì„¤ëª… ê²€ìƒ‰")
    
    # í•„í„°ë§ ì ìš©
    filtered_tools = filter_tools_by_difficulty(tools_data, selected_difficulty)
    filtered_tools = filter_tools_by_category(filtered_tools, selected_category)
    filtered_tools = filter_tools_by_search(filtered_tools, search_term)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë„êµ¬ ë¶„í¬ ì‹œê°í™”
    with st.expander("ğŸ“Š AI ë„êµ¬ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ê·¸ë˜í”„ë¡œ ë³´ê¸°", expanded=False):
        fig = visualize_category_distribution(tools_data)
        st.pyplot(fig)
    
    # í•„í„°ë§ëœ ë„êµ¬ ë¦¬ìŠ¤íŠ¸
    st.markdown("### ğŸ“‹ í•„í„°ë§ëœ ë„êµ¬ ëª©ë¡")
    if filtered_tools:
        tool_df = pd.DataFrame([{
            "ì´ë¦„": tool.get("name", ""),
            "ì¹´í…Œê³ ë¦¬": tool.get("category", ""),
            "ë‚œì´ë„": translate_difficulty(tool.get("difficulty", "medium"))
        } for tool in filtered_tools])
        
        st.dataframe(tool_df, use_container_width=True)
        
        # ë„êµ¬ ìƒì„¸ ì •ë³´ í™•ì¸
        selected_tool_name = st.selectbox("ìƒì„¸ ì •ë³´ë¥¼ ë³¼ ë„êµ¬ ì„ íƒ", ["ì„ íƒí•˜ì„¸ìš”"] + tool_df["ì´ë¦„"].tolist())
        
        if selected_tool_name != "ì„ íƒí•˜ì„¸ìš”":
            tool_info = get_tool_details(selected_tool_name, tools_data)
            if tool_info:
                st.markdown(f"### {selected_tool_name} ìƒì„¸ ì •ë³´")
                st.markdown(f"**ì¹´í…Œê³ ë¦¬**: {tool_info.get('category', 'ì •ë³´ ì—†ìŒ')}")
                st.markdown(f"**ë‚œì´ë„**: {translate_difficulty(tool_info.get('difficulty', 'medium'))}")
                
                # í•œêµ­ì–´ ì„¤ëª… ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ì„¤ëª… ì‚¬ìš©
                if tool_info.get("korean_description"):
                    st.markdown(f"**ê¸°ë³¸ ì„¤ëª…**: {tool_info.get('korean_description')}")
                elif tool_info.get("description"):
                    st.markdown(f"**ê¸°ë³¸ ì„¤ëª…**: {tool_info.get('description')}")
                else:
                    st.markdown("**ê¸°ë³¸ ì„¤ëª…**: ìƒì„¸ ì„¤ëª… ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                # AI ë„êµ¬ ì „ë¬¸ê°€ì˜ ì„¤ëª… ìƒì„± (ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬)
                try:
                    st.markdown("### ğŸ¤– AI ë„êµ¬ ì „ë¬¸ê°€ì˜ ìƒì„¸ ì„¤ëª…")
                    with st.spinner(f"{selected_tool_name}ì— ê´€í•œ ìƒì„¸ ì •ë³´ ë¶„ì„ ì¤‘..."):
                        # ìƒˆë¡œìš´ í•¨ìˆ˜ í˜¸ì¶œ (ì„¹ì…˜ë³„ ìƒì„±)
                        generate_expert_explanation_by_sections(selected_tool_name, qa, st)
                
                except Exception as e:
                    st.error(f"ì „ë¬¸ê°€ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.info("ì„ íƒí•œ ì¡°ê±´ì— ë§ëŠ” ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

#========== ì‚¬ìš©ì í”¼ë“œë°± ==========
st.markdown("---")
st.markdown("### ğŸ“ ì¶”ì²œ í”¼ë“œë°±")
feedback_tool = st.text_input("í”¼ë“œë°±ì„ ë‚¨ê¸¸ ë„êµ¬ ì´ë¦„")

if feedback_tool:
    rating = st.slider("ë§Œì¡±ë„ í‰ê°€", 1, 5, 3)
    feedback_text = st.text_area("ìƒì„¸ í”¼ë“œë°± (ì„ íƒì‚¬í•­)")
    
    if st.button("í”¼ë“œë°± ì œì¶œ"):
        if save_user_feedback(feedback_tool, rating, feedback_text):
            st.success("í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
        else:
            st.error("í”¼ë“œë°± ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

#========== PDF ê¸°ë°˜ AI ë„êµ¬ ì§ˆì˜ì‘ë‹µ ==========
st.markdown("---")
st.markdown("### ğŸ¤– AI ë„êµ¬ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê¸°")
st.write("AI ë„êµ¬ ì „ë¬¸ê°€ê°€ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

# ì„¸ì…˜ ìƒíƒœì— ì§ˆë¬¸-ë‹µë³€ ì €ì¥
if 'qa_history' not in st.session_state:
    st.session_state.qa_history = []

user_question = st.text_input("AI ë„êµ¬ì— ê´€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ChatGPTì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?")

if user_question:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            # ì‘ë‹µ ì‹œê°„ ì¸¡ì • ì‹œì‘
            start_time = time.time()
            
            # ì§ˆë¬¸ ì „ì²˜ë¦¬ (ìœ ë‹ˆì½”ë“œ ë¬¸ì ì²˜ë¦¬)
            clean_question = re.sub(r'[\u2014\u2013\u2015\u2017\u2018\u2019\u201a\u201b\u201c\u201d\u201e\u201f\u2020\u2021\u2026\u2032\u2033]+', '-', user_question)
            clean_question = clean_question.encode('ascii', errors='ignore').decode('ascii')
            
            # ì§ˆë¬¸ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            context_prompt = f"""
            ë‹¹ì‹ ì€ AI ë„êµ¬ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìˆ˜ì¤€ê³¼ ì§ì—…ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ê¹Šì´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
            
            ê°€ëŠ¥í•˜ë‹¤ë©´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
            1. ì§ì ‘ì ì¸ ì§ˆë¬¸ ë‹µë³€
            2. ì¶”ê°€ ìƒì„¸ ì •ë³´ë‚˜ íŒ
            3. ê´€ë ¨ ë„êµ¬ë‚˜ í™œìš©ë²• ì¶”ì²œ
            ì§ˆë¬¸: {clean_question}
            """
            
            # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì§ˆë¬¸ ì²˜ë¦¬
            answer = qa.run(context_prompt)
            
            # ì‘ë‹µ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
            response_time = time.time() - start_time
            
            # ê²°ê³¼ ì €ì¥
            qa_result = {
                "question": user_question,
                "answer": answer,
                "response_time": response_time,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "rated": False
            }
            
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.qa_history.append(qa_result)
            
            st.markdown("### ğŸ“ ë‹µë³€")
            st.markdown(answer)
            
            # ì‘ë‹µ ì‹œê°„ í‘œì‹œ
            st.caption(f"ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ")
            
            # ê´€ë ¨ ë¬¸ì„œ í‘œì‹œ
            docs = vectorstore.similarity_search(clean_question, k=2)
            
            with st.expander("ì°¸ê³  ìë£Œ", expanded=False):
                st.markdown("### ğŸ“„ ì°¸ê³ í•œ ë¬¸ì„œ")
                for i, doc in enumerate(docs):
                    st.markdown(f"**ì¶œì²˜ #{i+1} (í˜ì´ì§€ {doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')+1})**")
                    st.markdown(doc.page_content)
            
        except Exception as e:
            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ì´ì „ ì§ˆë¬¸-ë‹µë³€ ê¸°ë¡ í‘œì‹œ
if st.session_state.qa_history:
    with st.expander("ì´ì „ ì§ˆë¬¸ ê¸°ë¡", expanded=False):
        for i, qa_item in enumerate(reversed(st.session_state.qa_history[:-1] if user_question else st.session_state.qa_history)):
            st.markdown(f"**ì§ˆë¬¸ {i+1}**: {qa_item['question']}")
            st.markdown(f"**ë‹µë³€**: {qa_item['answer']}")
            st.caption(f"ì‘ë‹µ ì‹œê°„: {qa_item['response_time']:.2f}ì´ˆ | ì‹œê°„: {qa_item['timestamp']}")
            st.markdown("---")


st.markdown("---")
st.button("ğŸ”„ ì„¤ë¬¸ ë‹¤ì‹œ í•˜ê¸°", on_click=reset_survey)
